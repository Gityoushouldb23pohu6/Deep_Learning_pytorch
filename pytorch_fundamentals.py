# -*- coding: utf-8 -*-
"""pytorch_fundamentals.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gQfacsjW4lVLYl6VZXXXtVWG7Ftozshp
"""

!nvidia-smi

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
print(torch.__version__)

'''
tensors are the main building blocks of data

tensors -> numeric data

scaler


'''

# creating pytorch tensors
scaler = torch.tensor(7)
scaler                     #scalers are 0 ranked tensors

scaler.ndim

#get tensor back as python int
scaler.item()

#VECTOR
vector = torch.tensor([7,7])
vector

vector.ndim # no of pairs of closing square vector

vector.shape # number of elements

#MATRIX
MATRIX = torch.tensor([[7,8],[9,10]])
MATRIX

MATRIX.ndim

MATRIX[1]

MATRIX[1,1]

MATRIX.shape

#TENSOR
TENSOR= torch.tensor([[[1,2,3],
                       [3,6,9],
                       [2,4,5]]])
TENSOR

TENSOR.ndim

TENSOR.shape       #one tensor , 3 rows , 3 columns

TENSOR[0]

TENSOR[0,0]

TENSOR[0,1]

TENSOR[0,2,2]

TENSOR2=torch.tensor([[[[[1,2],[2,3],[4,5]]]]])

TENSOR2.shape

TENSOR2.ndim

TENSOR2[0]

TENSOR2[0,0]

TENSOR2[0,0,0]

TENSOR2

TENSOR2[0,0,0,0]

TENSOR2[0,0,0,0,1]

# A tensor can be of almost any size and shape and can carry in any dimensions of data

#Random tensors
'''
Random tensors are important because the way many neural networks learn is that
they start with tensors full of random numbers and then adjust those random numbers
to better represent the data .

Start with random numbers -> look at data -> update random numbers ->look at data ->update


'''

random_tensor=torch.rand(1,3,4) # random tensor of size (3,4)
random_tensor

random_tensor.ndim

# create a random tensor with similar shape to an image tensor

random_image_size_tensor = torch.rand(size=(224,224,3))   # in the format  height , width , and color forms

random_image_size_tensor.shape, random_image_size_tensor.ndim

rnd = torch.rand(3,4,4)
rnd

rnd.ndim

rnd2= torch.rand(5,2,2)
rnd2

# either size = () or directly () doesn't matter

# tensors of all zeros -> can be used for masking
zeros= torch.zeros(size=(3,4))
zeros

# CREATE a tensor of all ones
ones = torch.ones(size =(3,4))
ones

o3 = torch.ones(size=(6,9))
o3

ones.dtype

z2 = torch.zeros(size = (5,6))
z2

o2 = torch.ones(2,
                7,5)
o2



# Create a range of tensors and tensors -like

#use torch.range()

torch.arange(1,10)

torch.arange(0,2)

one_to_ten = torch.arange(start=1 , end = 11 , step =1
                          )
one_to_ten

# creating tensors like

ten_zeros = torch.zeros_like(one_to_ten)
ten_zeros

ten_to_twenty = torch.arange(start=10,end=21,step=1)
ten_to_twenty

all_zeros = torch.zeros_like(ten_to_twenty)
all_zeros

all_ones= torch.ones_like(input= ten_to_twenty)
all_ones

twenty_to_fifty = torch.arange(start=20,end=51,step=2)
twenty_to_fifty

twenty_to_fifty.ndim

twenty_to_fifty.shape

az=torch.zeros_like(twenty_to_fifty)
az

ao= torch.ones_like(twenty_to_fifty)
ao

"""**Tensor Datatypes **

"""

# float 32 tensor
float_32_tensor=torch.tensor([3.0,6.0,9.0],
                             dtype = None,# what dataype is it
                             device = 'cpu', # what device is tensor on
                             requires_grad=False ) # whether we want gradient to be tracked
float_32_tensor

float_32_tensor.dtype   # float 32 is the default data type in pytorch

float_16_tensor=torch.tensor([3.0,6.0,9.0],dtype = torch.float16)
float_16_tensor

float_16_tensor.dtype

# A 32 bit floating point is single precision and 16 bit is half the precision
 # *****if we want computations faster we might use 16 bit but if we want high accuracies we might go for 64 bit floating point
# now the precision means how much detail a single number has stored in the memory .

"""NOTE : tensor datatypes is one of the 3 big errors you'll run into with pytorch and deep learning

 1. tensors not right datatype
 2. not right shape
 3.  tensors not on right device

"""

'''
Device Parameter :
if we have two tensors on different devices and we are trying to do some kind of
operations between them then it would throw an error


Requires_grad :
if we want our pytorch to calculate the gradients as pytorch goes into computations

'''

float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor

float_32_tensor= float_16_tensor.type(torch.float32)
float_32_tensor

float_16_tensor * float_32_tensor # no error !!!

int_32_tensor = torch.tensor([3,6,9],dtype = torch.int32)

int_32_tensor

int_32_tensor * float_32_tensor

"""Getting Information From Tensor (Tensor Attributes )



Tensors not right datatype - to do get datatype from a tensor , can use 'tensor.dtype'

Tensors not right shape -to get shape from a tensor , can use tensor.shape

Tensors not on right device - to get device from a tensor , can use  tensor.device
"""

#Create a tensor

some_tensor= torch.rand(3,4)
some_tensor

# find details about tensor

print(some_tensor)

print(f'Datatype of tensor : {some_tensor.dtype}' )
print(f'shape of tensor : {some_tensor.shape}' )
print(f'Device tensor is on : {some_tensor.device}')


# size () and shape basically give the same thing
# all tensors created are by default in cpu

s_t= torch.rand(2,1)
s_t = s_t.type(torch.int32)
s_t

print(f'Data type : {s_t.dtype}')
print(f'Device : {s_t.device}')
print(f'Shape : {s_t.size()}')

# How do we change the device to gpu ?

"""MANIPULATING TENSORS :       
Tensor operations -

*addition
* subtraction
* multiplication (element wise)
* division
* matrix multiplication

"""

# create tensor

#add
tensor = torch.tensor ([1,2,3])
tensor+ 10

#multiply

tensor = tensor * 10
tensor

# subtract

tensor-100

#PYTORCH INBUILT  functions :

torch.mul(tensor,10)

torch.add(tensor,10)

"""Two main ways of performing multiplication in neural networks and deep learning


1* element wise
2* matrix multiplicaiton (dot product )


"""

# element wise :

print(tensor ,'*', tensor)
print(f'Equals : {tensor * tensor }')

#matrix multiplication

torch.matmul(tensor,tensor)

tensor

tensor = tensor/100

# mat mul by hand
1*1+2*2+3*3

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# value= 0
# for i in range(len(tensor)):
#   value +=tensor[i] * tensor[i]
# value
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.matmul(tensor,tensor)

# as we can clearly see , the time taken by the built in function is much less

"""ONE of the most common errors in deep learning :    
shape error


There are two main rules that performing matrix multiplication needs to satisfy :          


1 * the **inner dimentsions** must match

(3 ,2) @ (3,2) won't work
(2,3) @ (3,2) works

(3,2) @ (2,3) works

2 * The resulting matrix has a shape of outer dimensions :      


-(2,3) @(3,2) -> (2,2)
"""

torch.matmul(torch.rand(3,2),torch.rand(3,2)) #error

torch.rand(3,10) @ torch.rand(10,3)  # better to use matmul instead of this .

#shapes for matrix multiplication

tensor_A= torch.tensor(
    [[1,2],
     [3,4],
     [5,6]

    ]

)
tensor_B = torch.tensor([
    [7,10],
    [8,11],
    [9,12]



])
torch.mm(tensor_A,tensor_B)

tensor_A.shape , tensor_B.shape

"""To fix our tensor shape issues , we can manipulate the shape of oour tensors using a transpose . A transpose switched the axes or dimensions of a given tensor

"""

tensor_B.T, tensor_B.T.shape

tensor_B , tensor_B.shape

#The matrix multiplication operation works when tensor_B is transposed
print(f'originial shapes : tensor_A = {tensor_A.shape},tensor_B ={tensor_B.shape}')
print(f'New shapes : tensor_A = {tensor_A.shape} (same shape as above),tensor_B.T={tensor_B.T.shape}')
print(f'Multiplying : {tensor_A.shape} @ {tensor_B.T.shape}')
print('Output: \n')

output = torch.mm(tensor_A,tensor_B.T)
print(output)
print(f'\n output shape :{output.shape} ')
torch.matmul(tensor_A , tensor_B.T).shape

t_1 = torch.tensor([[1,2,3],[3,4,1]])
t_2=torch.tensor([[3,2],[1,2],[9,1]])

t_1.shape

t_2.size()

t_1 @ t_2

(t_1 @ t_2).shape

"""Finding the min , max , mean , sum etc (tensor aggregation)

"""

x= torch.arange(0,100,10)
print(x)
x.dtype  # long

torch.min(x) , x.min()

torch.max(x) , x.max()

torch.mean(x.type(torch.float32)), x.type(torch.float32).mean()  # the torch.mean() doesn't work with  int datatype

# note the torch.mean() function requires a tensor of float32 datatype to work

#Find the sum
torch.sum(x),x.sum()

x.type(torch.float32).mean()

"""FINDING THE POSITIONAL MIN AND MAX"""

x

#find the position in tensor that has the minimum value with argmin() -> returns the index position where the min value occurs
x.argmin()

#find the position in tensor that has the maximum value with argmax()
x.argmax()

x[9]

"""Reshaping , stacking ,squeezing and unsqueezing tensors

* Reshaping - reshapes and input tensor to a defined shape
* view -return a view of an input tensor of certain shape but keep the same memory as the original .
*stacking - combine multiple tensors on top of each other (vstack) or side by side (hstack)
* squeeze - removes all '1'dimensions from a tensor
* unsqueeze - add a 'l' dimension to a target tensor

* permute - return a view of input with dimensions permuted (swapped) in a certain way

"""

import torch
x=torch.arange(1.,11.)
x,x.shape

#add an extra dimension
x_reshaped = x.reshape(1,10)

x_reshaped ,   x_reshaped.shape
# the reshape has to be compatible with the original size

# change the view

z=x.view(10,1)
z,z.shape
# it sort of does the same thing as reshape

#changing z also changes x as they share the same  memory

z[:,0] = 5
z, x

#stack tensors on top of eachother
x_stacked = torch.stack([x,x,x,x],dim =1)
x_stacked

#torch.squeeze()-removes all single dimension from a target tensoe
x_reshaped

x_reshaped.shape

x_reshaped.squeeze()

x_reshaped.squeeze().shape

x=torch.arange(2,10)
x

print(f'previous tensor :  {x_reshaped}')
print(f'previous shape : {x_reshaped.shape}')

x_squeezed = x_reshaped.squeeze()
print(f"\nNew tensor : {x_squeezed}")
print(f"New shape : {x_squeezed.shape}")

#torch unsqueee() : adds a single dimension to a target tensor at a specific dim

print(f'previous target : {x_squeezed}')
print(f'previous shape : {x_squeezed.shape}')

# add an extra dimension with unsqueeze

x_unsqueezed = x_squeezed.unsqueeze(dim=0)
print(f'new shape : {x_unsqueezed.shape}')

print(x_unsqueezed )

# Torch permute -> rearrange dims of a target tensor in a specified order

x_original = torch.rand(size=(224,224,3)) # height , width , color_channels
                                          # h,w and col are encoded as 0 ,1 and 2
#permute the original tensor to rearrange the axis (or dim) order
x_permuted = x_original.permute(2,0,1) # we want color , height and then width dimension


# shifts axis 0->1 , 1->2 , 2->0
print(f'previous shape : {x_original.shape}')
print(f'New shape : {x_permuted .shape }')

# also permuted shares the same memory as the original tensor

x_original[0,0,0] = 29093

x_permuted[0,0,0]

x_permuted

"""## Indexing (selecting data from tensors )


indexing in pytorch is similar to indexing in numpy
"""

import torch
x= torch.arange(1,10).reshape(1,3,3)

x

x[0]

# index on middle bracket (dim=1)
x[0][0]

x[0,0]

#index on the most inner bracket

x[0][0][0]

x[1][1][1]

x[0][1][1]

x

x[0,2,2]

# you can use ':' to select all of the target dimension

x[:,0]

# get all values of 0th and 1st dim but only the index 1 of 2nd dimension

x[:,:,1]

# get all values of the 0 dimensions but only the 1 index value of the 1st and 2nd dimension
x[:,1,1]

# Get index 0 of 0th and 1st dimension and all values of 2nd dimension

x[0,0,:]   # or x[0][0]

#Index on x to return 3,6,9
x

x[:,:,2]

"""PYTORCH TENSORS AND NUMPY

Numpy is a popular scientific python numerical computing library

and because of this pytorch has functionalities to interact with it

* data in Numpy , want in pytorch tensor -> torch.from_numpy(ndarray)


* pytorch tensor ->Numpy -> torch.Tensor.numpy()


"""

#Numpy array to tensor

import torch
import numpy as np
array = np.arange(1.0,8.0)

tensor = torch.from_numpy(array)
array,tensor
# notice our default data type of torch is torch.float32 but there we are getting float 64 as type .
# this is because the default type of numpy is float 64 unless specified otherwise

array.dtype

tensor.dtype

torch.arange(1.0,8.0).dtype

#change the value of array
array
array= array+1
array

tensor  # tensor remains the same ***

#Tensor to numpy array

tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor


# now the data type of numpy tensor is going to be float 32 which is the default type of pytorch tensor

#change to tensor , what happens to numpy tensor -> no change

tensor = tensor+1

tensor,numpy_tensor

"""Reproducibility (trying to take random out of random )
In short how a neural network learns ðŸ‡°


'**start with random numbers ->tensor operations ->update random numbers to try and make them better representations of the data -> again ->again ->again ...  '**


To reduce the randomness in neural networks and Pytorch comes the concept of a **random seed **

Essentially what the random seed does is flavour the randomness


"""

torch.rand(3,3)  # everytime we get different random numbers

import torch

random_tensor_A = torch.rand(3,4)
random_tensor_B= torch.rand(3,4)

print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A== random_tensor_B)  # highly unlikely to get the same random numbers all the time

# Let's make some random but reproducible tensors

import torch

# set the random seed

RANDOM_SEED =42

torch.manual_seed(RANDOM_SEED)
random_tensor_C= torch.rand(3,4)
torch.manual_seed(RANDOM_SEED)
random_tensor_D = torch.rand(3,4)

print(random_tensor_C)
print(random_tensor_D)

print(random_tensor_C== random_tensor_D)


# if we are calling different random functions in one code block then we need to use torch.manual_seed everytime we call the function

"""##Running tensors and pytorch objects on the gpus (and making faster computations )

GPUs - faster computation on numbers , thanks to cuda +nvidia hardware +Pytorch working behind the scences to make everything good

1) Getting a GPU

-easiest - use google colab for gpu
- use own gpu with setup and investment of purchasing gpu
- use cloud computing -gcp , aws , azure

*2 ## check for gpu access with pytorch
"""

import torch
torch.cuda.is_available()

"""## setup device agnostic code"""

device ="cuda" if torch.cuda.is_available() else 'cpu'
device

## Count number of devices

torch.cuda.device_count()

"""For pytorch since its capable of running compute on the gpu or cpu , its best practice to setup device agnostic code

** Putting tensors (and models ) oon the gpu

The reason we want our tensors /models on the gpu is because using a gpu results in faster computations
"""

tensor = torch .tensor([1,2,3],device = 'cpu')
print(tensor,tensor.device)